import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import os
import math
import cv2
import numpy as np
from PIL import Image, ImageDraw

def auto_crop_product(image_path, output_path="cropped_product.png", margin=20, outline_thickness=5):
    """
    T·ª± ƒë·ªông crop ·∫£nh s·∫£n ph·∫©m v√† th√™m vi·ªÅn tr·∫Øng m∆∞·ª£t m√† bao quanh s·∫£n ph·∫©m.

    Args:
        image_path: ƒê∆∞·ªùng d·∫´n ·∫£nh ƒë·∫ßu v√†o
        output_path: ƒê∆∞·ªùng d·∫´n ·∫£nh ƒë·∫ßu ra
        margin: Kho·∫£ng c√°ch padding xung quanh s·∫£n ph·∫©m
        outline_thickness: ƒê·ªô d√†y vi·ªÅn tr·∫Øng (pixel)
    """
    img = Image.open(image_path).convert("RGBA")
    arr = np.array(img)

    # --- T·∫°o mask ---
    if arr.shape[2] == 4:  # N·∫øu c√≥ k√™nh alpha (·∫£nh x√≥a n·ªÅn)
        alpha = arr[:, :, 3]
        mask = alpha > 0
    else:
        # N·∫øu kh√¥ng c√≥ alpha, t·∫°o mask d·ª±a theo ƒë·ªô s√°ng (gi·∫£ s·ª≠ n·ªÅn tr·∫Øng)
        gray = cv2.cvtColor(np.array(img.convert("RGB")), cv2.COLOR_RGB2GRAY)
        mask = gray < 250

    # --- T√¨m bounding box ---
    coords = np.argwhere(mask)
    if coords.size == 0:
        print("‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m trong ·∫£nh.")
        return

    y0, x0 = coords.min(axis=0)
    y1, x1 = coords.max(axis=0)

    # --- Th√™m margin cho c√¢n ƒë·ªëi ---
    y0 = max(y0 - margin, 0)
    x0 = max(x0 - margin, 0)
    y1 = min(y1 + margin, arr.shape[0])
    x1 = min(x1 + margin, arr.shape[1])

    # --- Crop v√πng ch·ª©a s·∫£n ph·∫©m ---
    cropped = arr[y0:y1, x0:x1]

    # --- T·∫†O VI·ªÄN TR·∫ÆNG M∆Ø·ª¢T M√Ä BAO QUANH S·∫¢N PH·∫®M ---
    # L·∫•y mask c·ªßa v√πng crop
    if cropped.shape[2] == 4:
        crop_alpha = cropped[:, :, 3]
        crop_mask = (crop_alpha > 0).astype(np.uint8) * 255
    else:
        crop_gray = cv2.cvtColor(cropped[:, :, :3], cv2.COLOR_RGB2GRAY)
        crop_mask = (crop_gray < 250).astype(np.uint8) * 255

    # L√†m m·ªãn mask tr∆∞·ªõc ƒë·ªÉ gi·∫£m rƒÉng c∆∞a
    crop_mask_smooth = cv2.GaussianBlur(crop_mask, (5, 5), 0)
    _, crop_mask_smooth = cv2.threshold(crop_mask_smooth, 127, 255, cv2.THRESH_BINARY)

    # T√¨m contours c·ªßa s·∫£n ph·∫©m
    contours, _ = cv2.findContours(crop_mask_smooth, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # L√†m m·ªãn contour b·∫±ng approxPolyDP
    smooth_contours = []
    for contour in contours:
        epsilon = 0.001 * cv2.arcLength(contour, True)  # Gi·∫£m epsilon ƒë·ªÉ gi·ªØ chi ti·∫øt
        approx = cv2.approxPolyDP(contour, epsilon, True)
        smooth_contours.append(approx)

    # T·∫°o ·∫£nh l·ªõn h∆°n ƒë·ªÉ v·∫Ω vi·ªÅn v·ªõi anti-aliasing
    scale = 4  # TƒÉng k√≠ch th∆∞·ªõc l√™n 4 l·∫ßn
    h, w = cropped.shape[:2]
    large_img = cv2.resize(cropped, (w * scale, h * scale), interpolation=cv2.INTER_LINEAR)

    # Scale contours l√™n theo t·ª∑ l·ªá
    scaled_contours = [cnt * scale for cnt in smooth_contours]

    # V·∫Ω vi·ªÅn tr·∫Øng tr√™n ·∫£nh ph√≥ng to
    cv2.drawContours(large_img, scaled_contours, -1, (255, 255, 255, 255),
                     outline_thickness * scale, lineType=cv2.LINE_AA)

    # Thu nh·ªè l·∫°i v·ªÅ k√≠ch th∆∞·ªõc ban ƒë·∫ßu v·ªõi anti-aliasing
    cropped_with_outline = cv2.resize(large_img, (w, h), interpolation=cv2.INTER_AREA)

    # --- CƒÉn gi·ªØa tr√™n canvas vu√¥ng ---
    h, w = cropped_with_outline.shape[:2]
    size = max(h, w)
    canvas = np.zeros((size, size, 4), dtype=np.uint8)
    y_offset = (size - h) // 2
    x_offset = (size - w) // 2
    canvas[y_offset:y_offset+h, x_offset:x_offset+w] = cropped_with_outline

    # --- L∆∞u k·∫øt qu·∫£ ---
    Image.fromarray(canvas).save(output_path)
    # print(f"‚úÖ ·∫¢nh s·∫£n ph·∫©m ƒë√£ crop v·ªõi vi·ªÅn tr·∫Øng m∆∞·ª£t m√†: {output_path}")


# auto_crop_product(
#     "/content/Screenshot-2021-12-20-104958-Photoroom.png",
#     "product_cropped.png",
#     margin=15,
#     outline_thickness=10  # ƒê·ªô d√†y vi·ªÅn
# )
class VideoProductOverlay:
    def __init__(self, video_path, image_path, output_path, duration, text_content, font_path=None, font_size=80):
        """
        Kh·ªüi t·∫°o Video Product Overlay

        Args:
            video_path: ƒê∆∞·ªùng d·∫´n video n·ªÅn
            image_path: ƒê∆∞·ªùng d·∫´n ·∫£nh s·∫£n ph·∫©m (PNG v·ªõi n·ªÅn trong su·ªët)
            output_path: ƒê∆∞·ªùng d·∫´n video output
            duration: Th·ªùi l∆∞·ª£ng video (gi√¢y)
            text_content: N·ªôi dung text hi·ªÉn th·ªã
            font_path: ƒê∆∞·ªùng d·∫´n file font .ttf
            font_size: C·ª° ch·ªØ
        """
        self.video_path = video_path
        self.image_path = image_path
        self.output_path = output_path
        self.duration = duration
        self.text_content = text_content
        self.font_path = font_path
        self.font_size = font_size

        # Load video
        self.video = cv2.VideoCapture(video_path)
        self.video_fps = self.video.get(cv2.CAP_PROP_FPS)
        self.video_width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.video_height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.video_frame_count = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))

        # Load image
        self.product_image = Image.open(image_path).convert('RGBA')

        # Detect aspect ratio
        self.aspect_ratio = self.video_width / self.video_height
        self.is_16x9 = self.aspect_ratio > 1.5
        self.is_9x16 = self.aspect_ratio < 0.7

        # Ki·ªÉm tra th·ªùi l∆∞·ª£ng video
        self.video_duration = self.video_frame_count / self.video_fps
        if self.duration > self.video_duration:
            print(f"‚ö†Ô∏è C·∫£nh b√°o: Duration ({self.duration}s) d√†i h∆°n video ({self.video_duration:.2f}s)")
            print(f"   Video s·∫Ω ƒë∆∞·ª£c loop ƒë·ªÉ ƒë·ªß th·ªùi l∆∞·ª£ng")

        # Calculate timings
        self.move_in_duration = duration * 0.25      # Giai ƒëo·∫°n 1: Di chuy·ªÉn v√†o
        self.zoom_duration = duration * 0.15         # Giai ƒëo·∫°n 1.5: Zoom to-nh·ªè
        self.move_second_duration = duration * 0.15  # Giai ƒëo·∫°n 2: Di chuy·ªÉn ƒë·∫øn v·ªã tr√≠ cu·ªëi
        self.text_display_duration = duration * 0.45 # Giai ƒëo·∫°n 3: Hi·ªÉn th·ªã text

        # Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c√°c giai ƒëo·∫°n
        self.zoom_start = self.move_in_duration
        self.phase2_start = self.move_in_duration + self.zoom_duration
        self.phase3_start = self.phase2_start + self.move_second_duration

        # Text animation timing
        self.text_fade_in_duration = 0.3  # Th·ªùi gian fade in
        self.text_typing_speed = 0.05     # Gi√¢y m·ªói k√Ω t·ª± (t·ªëc ƒë·ªô ƒë√°nh m√°y)

        # T√≠nh to√°n th·ªùi gian typing
        text_length = len(self.text_content)
        self.text_typing_duration = text_length * self.text_typing_speed

        # Text fade out b·∫Øt ƒë·∫ßu g·∫ßn cu·ªëi video
        self.text_fade_out_start = self.duration - 0.5
        self.text_fade_out_duration = 0.5

        print(f"Video: {self.video_width}x{self.video_height} @ {self.video_fps} FPS")
        print(f"Video duration: {self.video_duration:.2f}s | Output duration: {self.duration}s")
        print(f"Aspect Ratio: {self.aspect_ratio:.2f}")
        print(f"Mode: {'16:9 (Landscape)' if self.is_16x9 else '9:16 (Portrait)' if self.is_9x16 else 'Other'}")
        print(f"Phase 1 (move in): 0s -> {self.zoom_start:.2f}s")
        print(f"Phase 1.5 (zoom pulse): {self.zoom_start:.2f}s -> {self.phase2_start:.2f}s")
        print(f"Phase 2 (move to final): {self.phase2_start:.2f}s -> {self.phase3_start:.2f}s")
        print(f"Phase 3 (wobble + text): {self.phase3_start:.2f}s -> {self.duration:.2f}s")
        print(f"Text typing duration: {self.text_typing_duration:.2f}s")

    def ease_in_out_cubic(self, t):
        """H√†m easing cho animation m∆∞·ª£t m√†"""
        return 4 * t * t * t if t < 0.5 else 1 - pow(-2 * t + 2, 3) / 2

    def ease_out_back(self, t):
        """H√†m easing v·ªõi hi·ªáu ·ª©ng bounce nh·∫π khi k·∫øt th√∫c"""
        c1 = 1.70158
        c3 = c1 + 1
        return 1 + c3 * pow(t - 1, 3) + c1 * pow(t - 1, 2)

    def get_zoom_scale(self, current_time):
        """
        T√≠nh to√°n scale cho hi·ªáu ·ª©ng zoom to-nh·ªè (pulse)

        Args:
            current_time: Th·ªùi gian hi·ªán t·∫°i
        Returns:
            float: Scale factor (1.0 = k√≠ch th∆∞·ªõc g·ªëc)
        """
        if current_time < self.zoom_start or current_time >= self.phase2_start:
            return 1.0

        # Th·ªùi gian trong giai ƒëo·∫°n zoom
        elapsed = current_time - self.zoom_start
        t = elapsed / self.zoom_duration

        # T·∫°o hi·ªáu ·ª©ng zoom: to ra -> nh·ªè l·∫°i -> v·ªÅ b√¨nh th∆∞·ªùng
        zoom_factor = 0.15  # ƒê·ªô l·ªõn c·ªßa zoom (15% l·ªõn h∆°n)
        scale = 1.0 + (math.sin(t * math.pi * 2) * zoom_factor * (1 - t))

        return scale

    def get_text_alpha_and_length(self, current_time):
        """
        T√≠nh to√°n ƒë·ªô trong su·ªët v√† s·ªë k√Ω t·ª± hi·ªÉn th·ªã c·ªßa text

        Args:
            current_time: Th·ªùi gian hi·ªán t·∫°i
        Returns:
            tuple: (alpha, visible_chars)
                - alpha: 0.0 ƒë·∫øn 1.0
                - visible_chars: s·ªë k√Ω t·ª± hi·ªÉn th·ªã
        """
        if current_time < self.phase3_start:
            return 0.0, 0

        elapsed = current_time - self.phase3_start
        text_length = len(self.text_content)

        # FADE IN
        if elapsed < self.text_fade_in_duration:
            alpha = elapsed / self.text_fade_in_duration
            alpha = self.ease_in_out_cubic(alpha)  # Smooth fade
            visible_chars = 0
            return alpha, visible_chars

        # TYPING EFFECT
        typing_elapsed = elapsed - self.text_fade_in_duration
        if typing_elapsed < self.text_typing_duration:
            alpha = 1.0
            visible_chars = int((typing_elapsed / self.text_typing_duration) * text_length)
            visible_chars = min(visible_chars, text_length)
            return alpha, visible_chars

        # FULLY VISIBLE
        if current_time < self.text_fade_out_start:
            return 1.0, text_length

        # FADE OUT
        fade_out_elapsed = current_time - self.text_fade_out_start
        if fade_out_elapsed < self.text_fade_out_duration:
            alpha = 1.0 - (fade_out_elapsed / self.text_fade_out_duration)
            alpha = self.ease_in_out_cubic(alpha)  # Smooth fade
            return alpha, text_length

        return 0.0, text_length

    def get_wobble_offset(self, current_time, start_time, fade_in_duration=0.3):
        """
        T√≠nh to√°n ƒë·ªô l·ªách cho hi·ªáu ·ª©ng nh·∫•p nh√¥ nh∆∞ s√≥ng v·ªõi fade-in m∆∞·ª£t

        Args:
            current_time: Th·ªùi gian hi·ªán t·∫°i
            start_time: Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu wobble
            fade_in_duration: Th·ªùi gian fade-in c·ªßa wobble (gi√¢y)
        """
        frequency = 1.5  # Hz - t·∫ßn s·ªë s√≥ng ch·∫≠m
        amplitude = 8    # pixels - bi√™n ƒë·ªô l√™n xu·ªëng

        elapsed = current_time - start_time
        if elapsed < 0:
            return 0, 0

        # Fade-in m∆∞·ª£t cho wobble
        fade_multiplier = min(1.0, elapsed / fade_in_duration)
        fade_multiplier = self.ease_in_out_cubic(fade_multiplier)

        # T√≠nh offset theo sin wave - CH·ªà DI CHUY·ªÇN THEO TR·ª§C Y
        angle = elapsed * frequency * 2 * math.pi
        offset_y = math.sin(angle) * amplitude * fade_multiplier

        offset_x = 0

        return int(offset_x), int(offset_y)

    def resize_product_image(self, scale=1.0):
        """
        Resize ·∫£nh s·∫£n ph·∫©m v·ªõi scale factor

        Args:
            scale: H·ªá s·ªë zoom (1.0 = k√≠ch th∆∞·ªõc g·ªëc)
        """
        base_size = 512
        new_size = int(base_size * scale)
        return self.product_image.resize((new_size, new_size), Image.Resampling.LANCZOS)

    def get_image_position(self, current_time, img_w, img_h):
        """T√≠nh to√°n v·ªã tr√≠ ·∫£nh d·ª±a tr√™n th·ªùi gian"""
        center_x = (self.video_width - img_w) // 2
        center_y = (self.video_height - img_h) // 2
        show_text = False
        wobble_x, wobble_y = 0, 0

        if self.is_16x9:
            # Animation 16:9: top ‚Üí center ‚Üí ZOOM ‚Üí left + text
            final_x = center_x - int(self.video_width * 0.15)

            if current_time < self.zoom_start:
                # GIAI ƒêO·∫†N 1: Di chuy·ªÉn t·ª´ tr√™n xu·ªëng gi·ªØa
                t = current_time / self.move_in_duration
                ease_t = self.ease_in_out_cubic(t)
                img_x = center_x
                img_y = int(-img_h + (center_y + img_h) * ease_t)

            elif current_time < self.phase2_start:
                # GIAI ƒêO·∫†N 1.5: ZOOM TO-NH·ªé t·∫°i center
                img_x = center_x
                img_y = center_y

            elif current_time < self.phase3_start:
                # GIAI ƒêO·∫†N 2: Di chuy·ªÉn sang tr√°i
                t = (current_time - self.phase2_start) / self.move_second_duration
                ease_t = self.ease_out_back(t)
                img_x = int(center_x - (center_x - final_x) * ease_t)
                img_y = center_y

                # B·∫ÆT ƒê·∫¶U WOBBLE
                wobble_x, wobble_y = self.get_wobble_offset(current_time, self.phase2_start, fade_in_duration=0.5)

            else:
                # GIAI ƒêO·∫†N 3: V·ªã tr√≠ cu·ªëi c√πng v·ªõi text + wobble
                img_x = final_x
                img_y = center_y
                show_text = True
                wobble_x, wobble_y = self.get_wobble_offset(current_time, self.phase2_start)

        elif self.is_9x16:
            # Animation 9:16: left ‚Üí center ‚Üí ZOOM ‚Üí down + text
            final_y = center_y + int(self.video_height * 0.15)

            if current_time < self.zoom_start:
                # GIAI ƒêO·∫†N 1: Di chuy·ªÉn t·ª´ tr√°i v√†o gi·ªØa
                t = current_time / self.move_in_duration
                ease_t = self.ease_in_out_cubic(t)
                img_x = int(-img_w + (center_x + img_w) * ease_t)
                img_y = center_y

            elif current_time < self.phase2_start:
                # GIAI ƒêO·∫†N 1.5: ZOOM TO-NH·ªé t·∫°i center
                img_x = center_x
                img_y = center_y

            elif current_time < self.phase3_start:
                # GIAI ƒêO·∫†N 2: Di chuy·ªÉn xu·ªëng d∆∞·ªõi
                t = (current_time - self.phase2_start) / self.move_second_duration
                ease_t = self.ease_out_back(t)
                img_x = center_x
                img_y = int(center_y + (final_y - center_y) * ease_t)

                # B·∫ÆT ƒê·∫¶U WOBBLE
                wobble_x, wobble_y = self.get_wobble_offset(current_time, self.phase2_start, fade_in_duration=0.5)

            else:
                # GIAI ƒêO·∫†N 3: V·ªã tr√≠ cu·ªëi c√πng v·ªõi text + wobble
                img_x = center_x
                img_y = final_y
                show_text = True
                wobble_x, wobble_y = self.get_wobble_offset(current_time, self.phase2_start)

        else:
            # Default: center
            img_x = center_x
            img_y = center_y
            if current_time > self.phase2_start:
                show_text = True
                wobble_x, wobble_y = self.get_wobble_offset(current_time, self.phase2_start)

        # √Åp d·ª•ng hi·ªáu ·ª©ng rung
        img_x += wobble_x
        img_y += wobble_y

        return img_x, img_y, show_text

    def overlay_image(self, background, overlay, x, y):
        """Ch√®n ·∫£nh PNG c√≥ alpha l√™n background"""
        # Convert background to PIL
        bg_pil = Image.fromarray(cv2.cvtColor(background, cv2.COLOR_BGR2RGB))

        # Paste overlay
        bg_pil.paste(overlay, (x, y), overlay)

        # Convert back to OpenCV
        return cv2.cvtColor(np.array(bg_pil), cv2.COLOR_RGB2BGR)

    def wrap_text(self, text, font, max_width):
        """Chia text th√†nh nhi·ªÅu d√≤ng n·∫øu qu√° d√†i"""
        words = text.split()
        lines = []
        current_line = []

        for word in words:
            test_line = ' '.join(current_line + [word])
            # T·∫°o temporary draw ƒë·ªÉ test k√≠ch th∆∞·ªõc
            temp_img = Image.new('RGB', (1, 1))
            temp_draw = ImageDraw.Draw(temp_img)
            bbox = temp_draw.textbbox((0, 0), test_line, font=font)
            test_width = bbox[2] - bbox[0]

            if test_width <= max_width:
                current_line.append(word)
            else:
                if current_line:
                    lines.append(' '.join(current_line))
                current_line = [word]

        if current_line:
            lines.append(' '.join(current_line))

        return lines

    def draw_text(self, frame, text, show_text, current_time):
        """V·∫Ω text l√™n frame v·ªõi fade in/out v√† typing effect"""
        if not show_text or not text.strip():
            return frame

        # L·∫•y alpha v√† s·ªë k√Ω t·ª± hi·ªÉn th·ªã
        alpha, visible_chars = self.get_text_alpha_and_length(current_time)

        if alpha <= 0.0:
            return frame

        # Convert to PIL for text drawing
        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        # T·∫°o layer trong su·ªët cho text
        text_layer = Image.new('RGBA', pil_img.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(text_layer)

        # Try to use custom font or fallback
        try:
            if self.font_path and os.path.exists(self.font_path):
                font = ImageFont.truetype(self.font_path, self.font_size)
            else:
                font = ImageFont.truetype("arial.ttf", self.font_size)
        except:
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", self.font_size)
            except:
                font = ImageFont.load_default()

        # Calculate text area and position
        if self.is_16x9:
            text_area_x = int(self.video_width * 0.55)
            text_area_width = int(self.video_width * 0.4)
            text_y_center = self.video_height // 2
        elif self.is_9x16:
            text_area_x = int(self.video_width * 0.1)
            text_area_width = int(self.video_width * 0.8)
            text_y_center = int(self.video_height * 0.25)
        else:
            text_area_x = int(self.video_width * 0.1)
            text_area_width = int(self.video_width * 0.8)
            text_y_center = int(self.video_height * 0.8)

        # L·∫•y text hi·ªÉn th·ªã (typing effect)
        display_text = text[:visible_chars] if visible_chars > 0 else ""

        if not display_text:
            return frame

        # Wrap text n·∫øu qu√° d√†i
        lines = self.wrap_text(display_text, font, text_area_width)

        # Calculate line height
        bbox = draw.textbbox((0, 0), "Test", font=font)
        line_height = bbox[3] - bbox[1] + 10

        # Calculate total text block height
        total_height = line_height * len(lines)

        # Starting Y position (centered)
        start_y = text_y_center - (total_height // 2)

        # T√≠nh alpha cho m√†u
        alpha_int = int(255 * alpha)

        # Draw each line
        outline_width = 3
        for i, line in enumerate(lines):
            # Get line dimensions
            bbox = draw.textbbox((0, 0), line, font=font)
            line_width = bbox[2] - bbox[0]

            # Center line horizontally
            text_x = text_area_x + (text_area_width - line_width) // 2
            text_y = start_y + i * line_height

            # Draw text outline v·ªõi alpha
            for adj_x in range(-outline_width, outline_width + 1):
                for adj_y in range(-outline_width, outline_width + 1):
                    draw.text((text_x + adj_x, text_y + adj_y), line, font=font,
                             fill=(0, 0, 0, alpha_int))

            draw.text((text_x, text_y), line, font=font,
                     fill=(255, 255, 255, alpha_int))

        pil_img = pil_img.convert('RGBA')
        pil_img = Image.alpha_composite(pil_img, text_layer)
        pil_img = pil_img.convert('RGB')

        return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)

    def process(self):

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(self.output_path, fourcc, self.video_fps,
                             (self.video_width, self.video_height))

        total_frames = int(self.duration * self.video_fps)
        video_duration = self.video_frame_count / self.video_fps

        print(f"Processing {total_frames} frames...")

        for frame_idx in range(total_frames):
            # Calculate current time
            current_time = frame_idx / self.video_fps
            progress = (frame_idx / total_frames) * 100

            # Get video frame (loop if needed)
            video_time = (current_time / self.duration) * video_duration
            self.video.set(cv2.CAP_PROP_POS_MSEC, video_time * 1000)
            ret, frame = self.video.read()

            if not ret:
                self.video.set(cv2.CAP_PROP_POS_FRAMES, 0)
                ret, frame = self.video.read()

            # Get zoom scale v√† resize image theo scale
            zoom_scale = self.get_zoom_scale(current_time)
            resized_product = self.resize_product_image(zoom_scale)
            img_w, img_h = resized_product.size

            # Get image position
            img_x, img_y, show_text = self.get_image_position(current_time, img_w, img_h)

            # Overlay product image
            frame = self.overlay_image(frame, resized_product, img_x, img_y)

            # Draw text v·ªõi fade in/out v√† typing effect
            frame = self.draw_text(frame, self.text_content, show_text, current_time)

            # Write frame
            out.write(frame)

            if frame_idx % 30 == 0:
                print(f"Progress: {progress:.1f}% ({frame_idx}/{total_frames} frames)")

        # Release resources
        self.video.release()
        out.release()

        print(f"\n‚úÖ Video type 6 ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng: {self.output_path}")


def fairyending(VIDEO_FOLDER,FONT_FOLDER,IMAGE_PATH,OUTPUT_PATH,DURATION,FONT_SIZE):
    import os
    import random

    # === CONFIG PATHS ===
    # VIDEO_FOLDER = "/content/drive/MyDrive/1. Anymate me/16_9"   # Th∆∞ m·ª•c ch·ª©a nhi·ªÅu video
    # FONT_FOLDER = "/content/drive/MyDrive/1. Anymate me/font"     # Th∆∞ m·ª•c ch·ª©a nhi·ªÅu font
    # IMAGE_PATH = "/content/product_cropped (2).png"
    # OUTPUT_PATH = "output_video_smooth.mp4"
    # DURATION = 10
    # FONT_SIZE = 80

    # === RANDOM CHOICE ===
    def get_random_file(folder, extensions):
        files = [os.path.join(folder, f) for f in os.listdir(folder)
                if any(f.lower().endswith(ext) for ext in extensions)]
        if not files:
            raise FileNotFoundError(f"No valid files found in {folder}")
        return random.choice(files)

    # L·∫•y video v√† font ng·∫´u nhi√™n
    VIDEO_PATH = get_random_file(VIDEO_FOLDER, [".mp4", ".mov", ".avi"])
    FONT_PATH = get_random_file(FONT_FOLDER, [".ttf", ".otf"])

    # === RANDOM SLOGAN LIST ===
    SLOGANS = [
        "Creating something amazing every day.",
        "Bringing ideas to life, one frame at a time.",
        "Where innovation meets inspiration.",
        "Make it shine. Make it unforgettable.",
        "Designed to inspire confidence.",
        "Crafted with passion. Built for you.",
        "Simple. Elegant. Powerful.",
        "Turning imagination into reality.",
        "Every detail matters."
    ]

    TEXT_CONTENT = random.choice(SLOGANS)

    # === PRINT CHECK ===
    # print(f"üé¨ VIDEO_PATH: {VIDEO_PATH}")
    # print(f"üñã FONT_PATH: {FONT_PATH}")
    # print(f"üí¨ TEXT_CONTENT: {TEXT_CONTENT}")

    if not os.path.exists(VIDEO_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y video: {VIDEO_PATH}")
        return

    if not os.path.exists(IMAGE_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh: {IMAGE_PATH}")
        return
    import time

    filename = f"image_{int(time.time())}.png"
    auto_crop_product(
        IMAGE_PATH,
        filename,
        margin=15,
        outline_thickness=10  
    )
    overlay = VideoProductOverlay(
        video_path=VIDEO_PATH,
        image_path=filename,
        output_path=OUTPUT_PATH,
        duration=DURATION,
        text_content=TEXT_CONTENT,
        font_size=FONT_SIZE,
        font_path=FONT_PATH
    )

    overlay.process()
    return OUTPUT_PATH
